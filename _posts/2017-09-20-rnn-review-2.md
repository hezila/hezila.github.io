---
layout: post
title: The review of Recurrant Neural Network [2/2]
tags: [rnn, deeplearning]
keywords: [rnn]
---

Modern RNN architectures
------------------------

The most successful RNN architecture for sequence learning stem from two work published in 1997. The first, *Long Short-Term Memory*, introduces the memory cell, a unit of computation that replaces traditional nodes in the hidden layer of a network. With these memory cells, networks are able to overcome difficulties with training encounterred by earlier recurrent network. The second network, *Bidirectional Recurrent Neural Network*, introduces an architecture in which information from both the future and the past are used to determine the output at any point in the sequence. 


**Long short-term memory (LSTM)**

The LSTM model is introduced primarily in order to overcome the problem of vanishing gradients. This model resembles a standard recurrent neural network with a hidden layer, but each oridinary node in the hidden layer is replaced by a *memory cell*. Each memory cell contains a node with a self-connected recurrent edge of fixed weight one, ensuring that the gradient can pass across many time steps without vanishing or exploding. To distinguish references to a memory cell and not an ordinary node, we use the subscript $$c$$.















