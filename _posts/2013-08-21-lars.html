---
layout: post
title: Least Angle Regression
categories: [Machine Learning]
tags: [Machine Learning, Math]
---

<h2>背景知识</h2>
<p>
最小角回归和模型选择比较像，是一个逐步的过程，每一步都选择一个相关性最大的特征，总的运算步数只和特征的数目有关，和训练集的大小无关。最小角回归训练时的输入为特征矩阵 $$$ X = \{X_1, X_2,..., X_P\} $$$，和期输出向量$$$ Y = \{y_1, y_2,..., y_N \} $$$，$$$X_i$$$ 是长度为$$$N$$$的矩阵，$$$N$$$表示训练集的大小，$$$P$$$则是特征的数目。还有一点需要注意的是，向量$$$X_i$$$ 和 $$$Y$$$ 都是正则化之后的向量，即它们的元素的均值为0，且每个向量的长度都是1，这样做的目的是为了后面计算相关性以及角度的方便。
</p>

<h3>相关性</h3>
<p>
相关性一般是用来衡量两个向量之间的相关程度，通常采用相关性公式进行计算，其中A，B为向量：
</p>

$$ \begin{eqnarray*}
	corr &=& \frac{Cov(A,B)}{\sqrt{Var(A)Var(B)}} \\
		 &=& \frac{E[(A-\bar{A})(B-\bar{B})]}{\sqrt{E[(A-\bar{A})^2]E[(B-\bar{B})^2]}}
\end{eqnarray*} $$

<p>
corr的绝对值越大，表示A，B的相关性越大，反之则越小。corr的符号则表示这两个向量是正相关还是负相关。由于最小回归的训练数据是经过正则化的，即$$$ \bar{A} = \bar{B} = 0 $$$，$$$|A| = |B| = 1$$$，所以上面的计算公式可以进行简化。
</p>

$$ \begin{eqnarray*}
	corr &=& \frac{E(AB)}{\sqrt{E(A^2)E(B^2)}} \\
		 &=& \frac{E(AB)}{\sqrt{1*1}} \\
		 &=& E(AB)
\end{eqnarray*} $$

<p>
在最小角回归中，$$$E(AB) = \frac{AB}{n} $$$，其中n是向量A或B中元素的个数，因而n是定值，所以为了方便计算，在计算向量的相关性时，我们只需要计算$$$ AB $$$即可。为了计算每个特征和期望输出的相关性，我们需要计算每个$$$X_i$$$ 与 $$$Y$$$的相关性，这种情况下，我们可以采用矩阵和向量的乘法来进行运算，这里X是$$$N \times P$$$的特征矩阵，Y则是大小为N的期望输出向量。
</p>

$$ \begin{eqnarray*}
	Corr &=& X^T Y
\end{eqnarray*} $$

<p>
这里的$$$Corr$$$是一个大小为P的向量，且 $$$Corr_i$$$表示特征$$$X_i$$$与$$$Y$$$的相关性。
</p>

<h2>参数选择过程</h2>
<p>
在最小角回归中，参数选择的准则就是相关性，每次都选择和期望输出相关性最大的特征，即$$$max(|Corr_i|)$$$。不过需要注意的是，在具体计算时，期望输出并不总是Y，而是Y与已选择路径的差向量。
</p>

<h3>前进路径</h3>
<p>
在我们的特征矩阵中，共有P个特征向量，每个向量都是N维的，期望输出矩阵Y也是N维的，因此我们可以将这些特征向量以及期望输出向量看作是N维空间中的点，而我们的目的就是在这个空间中找到一条从原点到Y的路径。需要满足的条件就是这条路径可以使用特征矩阵的线性组来合表示，而这些特征矩阵的系数就是线性回归的权重向量。
</p>

<p>
$$$ \hat{u}_A$$$ 表示从原点出发的一条路径，其初始值为$$$ \vec{0}$$$，我们的目的是在经过若干步之后，使得$$$ \hat{u}_A = Y $$$。每一步中，$$$\hat{u}_A$$$的增量为 $$$\hat{\gamma}u_A$$$，其中$$$u_A$$$ 为单位向量，它指明了 $$$\hat{u}_A$$$ 的前进方向，$$$\hat{\gamma}$$$ 则是前进的距离。
</p>

<h3>前进方向</h3>
<p>
在最小角回归的计算过程中，每一步都会选择一个和期望输出$$$ Y - \hat{u_A}$$$（也可以称作残值），最相关的特征$$$X_i$$$，可以用矩阵 $$$ X_A$$$ 表示被选中的特征的集合，$$$ X_A = (...,s_jX_j,...)$$$，$$$s_j$$$ 表示$$$X_j$$$ 与期望输出相关性的符号。选择完变量之后，就是计算前进方向。由于是最小角回归，所以其选择方向的策略就是选择等分$$$X_A$$$中所有向量的角的方向，也就是说$$$u_A$$$和$$$X_A$$$中所有的列向量所成的角相等，那么只需要保证他们之间的$$$cos$$$值相等即可。因为$$$ cos(A,B) = \frac{AB}{|A||B|}$$$，而所有的$$$ |s_jX_j| = 1$$$， 因此为了保证所有的$$$ cos(s_jX_j, u_A) $$$都相等，只需要保证所有的$$$(s_jX_j)u_A $$$ 都相等即可，也就是说要保证$$$ X_A^Tu_A = A_A1_A$$$，其中$$$A_A$$$为实数。我们知道方阵与其逆矩阵的乘积为单位矩阵，我们可以利用这个性质来构造$$$u_A$$$，然而我们无法保证$$$X_A^T$$$是方阵，但我们可以保证$$$X_A^T X_A$$$为方阵，因此我们可以令 $$$u_A = X_A(X_A^T X_A)^{-1}A_A1_A$$$，此时$$$X_A^T u_A = A_A1_A$$$。同时，因为$$$u_A$$$是单位向量，因此 $$$ |u_A| = 1$$$，也就是说 $$$ u_A^T u_A = 1$$$，由此可以得到 $$$ A_A = (1_A^T (X_A^T X_A)^{-1} 1_A)^{0.5}$$$。至此，我们得到以下公式:

$$ \begin{eqnarray*}
	G_A &=& X_A^T X_A \\
	A_A &=& (1_A^T G_A^{-1} 1_A)^{\frac{1}{2}} \\
	u_A &=& X_A G_A^{-1} A_A 1_A

\end{eqnarray*} $$
</p>

<h3>前进距离</h3>
<p>
在确定了前进方向之后，我们就可以沿着$$$u_A$$$的方向前进了，但应该前进多少呢？这是我们现在需要解决的问题。一直前进，直到有没被选中的向量与$$$(Y - \hat{u}_A - \hat{\gamma}u_A)$$$ 所成角的绝对值等于选中的向量与 $$$(Y-\hat{u}_A-\hat{\gamma}u_A)$$$ 所成角的绝对值。这里涉及到上面提到的相关性计算以及特征的选择，之前我们说在最小角回归中，每一步都会选择一个变量，在实际操作中，我们并不是在每一步中只选择一个变量，而是选中所有相关性的绝对值等于最大值的特征，不过这个操作和每一步选择一个变量的效果是一样，后面会有解释。在每一步中，我们具体的计算过程如下：
</p>

<ul>
<li>
计算特征向量和残值的相关性，并选择最相关的特征向量组成相关特征矩阵$$$X_A$$$

$$ \begin{eqnarray*}
	   Corr &=& X^T(Y - \hat{u_A}) \\
	\hat{C} &=& max_j{|Corr_j|}  \\
	    s_j &=& sign(Corr_j)  \\
	      A &=& \{j : |Corr_j| = \hat{C} \} \\
		X_A &=& \{s_jX_j : j \in A \}

\end{eqnarray*} $$
</li>

<li>
计算前进方向，上面已经经过，至此，我们得到了$$$u_A$$$
</li>

<li>
计算前进距离，上面已经讲过前进的终止条件，那么在计算中如何体现呢？即存在$$$X_j$$$其和当前残值所成角的绝对值等于被选中的特征于残值所成的角,为了方便计算，我们可以先计算 $$$ a = X^Tu_A $$$，同时我们令 $$$ A^c = \{1, 2,...,P\} - A$$$。

$$ \begin{eqnarray*}
	|X_{Ai}^T(Y-\hat{u}_A - \hat{\gamma}u_A)| &=& |X_j^T(Y-\hat{u}_A-\hat{\gamma}u_A)| \\
    \hat{C} - \hat{\gamma}A_A &=& |Corr_j - \hat{\gamma}a_j| \\
	\hat{\gamma} &=& min^+_{j \in A^c} \left\{\frac{\hat{C}-Corr_j}{A_A-a_j}, \frac{\hat{C}+Corr_j}{A_A+a_j} \right\}
\end{eqnarray*} $$

min右上角的+号，表示取最小的正数。
</li>

</ul>

<h3>为什么说每步选择一个参数</h3>
<p>
在最小角回归的文字描述中，我们说每一步都会选择相关性最大的特征加入到$$$X_A$$$中，而在实际计算时，我们却在每一步都重新构建了$$$X_A$$$，那这两种方式得到的矩阵$$$X_A$$$是否相等呢？实际上它们是相等，下面我们就可以来进行验证。
</p>

<p>
假设在第k步中选中了m个特征$$$X_{A1}, X_{A2},...,X_{Am}$$$被选中，此时我们需要验证就是在第k+1步之后，原来被选中的特征还需要被选中。在第k中，$$$\hat{u}_{Ak}$$$沿着$$$u_{Ak}$$$的方向前进了$$$\hat{\gamma}_k$$$距离，因此在第k+1步中，$$$\hat{u}_{A(k+1)} = \hat{u}_{Ak}+\hat{\gamma}_ku_{Ak}$$$。由于在第k步的前进过程中，$$$X_{Ai}$$$与$$$Y-\hat{u}_{Ak}-\hat{\gamma}u_A$$$所成角的绝对值都相等，其逐渐减小，$$$X_j$$$与残值所成角的绝对值则逐渐增大，一旦$$$X_{Ai}$$$与残值所成角的绝对值等于$$$X_j$$$与残值所成角的绝对值，立即停止前进。因此$$$X_j$$$与残值所成角的绝对值是所有未被选中的特征中与残值所成角的绝对值最大的特征，所以在第k+1步中
</p>

$$ \begin{eqnarray*}
	Corr &=& X^T(Y - \hat{u}_{k+1}) \\
	     &=& X^T(Y - \hat{u}_k - \hat{\gamma}_k u_{Ak}) \\
\end{eqnarray*} $$

<p>
其中绝对值最大的必然是第k步中被选中的特征以及$$$X_j$$$，由此我们知道第k步选中的特征在第k+1步中仍然会被选中，因而这两个方式构建的$$$X_A$$$是等价的，且有如下关系
</p>

$$ \begin{eqnarray*}
	\hat{C}_{k+1} &=& \hat{C}_k - \hat{\gamma}_k A_{Ak}

\end{eqnarray*} $$
